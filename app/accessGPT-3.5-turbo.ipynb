{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai cohere tiktoken -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Please enter your OpenAI API Key: \")\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-Ap2rQaRilOg74cHo0UsOng3eh3WNL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='LangChain and LlamaIndex are two distinct platforms that serve different purposes in the blockchain and cryptocurrency space.\\n\\nLangChain is a decentralized language service platform that aims to provide blockchain-based translation and interpretation services. It connects clients with translators and interpreters in a secure and efficient manner, using smart contracts to automate the payment and verification process. LangChain focuses on language-related services and aims to bridge the communication gap between different languages and cultures using blockchain technology.\\n\\nOn the other hand, LlamaIndex is a cryptocurrency price tracking and portfolio management platform. It allows users to track the prices of various cryptocurrencies in real-time, manage their investment portfolios, and analyze market trends. LlamaIndex provides valuable tools and information for cryptocurrency investors to make informed decisions and stay updated on the latest market developments.\\n\\nIn summary, LangChain is focused on language services leveraging blockchain technology, while LlamaIndex is centered around cryptocurrency price tracking and portfolio management.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736729088, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=184, prompt_tokens=19, total_tokens=203, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUR_PROMPT = \"What is the difference between LangChain and LlamaIndex?\"\n",
    "\n",
    "client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\" : \"user\", \"content\" : YOUR_PROMPT}]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_response(client: OpenAI, messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    return client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "def system_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"system\", \"content\": message}\n",
    "\n",
    "def assistant_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"assistant\", \"content\": message}\n",
    "\n",
    "def user_prompt(message: str) -> dict:\n",
    "    return {\"role\": \"user\", \"content\": message}\n",
    "\n",
    "def pretty_print(message: str) -> str:\n",
    "    display(Markdown(message.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "LangChain and LlamaIndex are two different projects operating in the blockchain and cryptocurrency space. \n",
       "\n",
       "1. LangChain: LangChain is a blockchain platform that focuses on providing language services such as translation, interpretation, and language learning through smart contracts. It aims to connect language professionals with clients in a decentralized manner, eliminating intermediaries and reducing costs. LangChain uses blockchain technology to ensure transparency, security, and efficiency in language-related transactions.\n",
       "\n",
       "2. LlamaIndex: LlamaIndex, on the other hand, is a decentralized finance (DeFi) platform that offers various financial services such as lending, borrowing, staking, and liquidity provision. It allows users to earn interest on their crypto assets, access loans using their digital assets as collateral, and participate in yield farming activities. LlamaIndex aims to provide users with more opportunities to generate passive income and grow their wealth through DeFi products.\n",
       "\n",
       "In summary, LangChain focuses on language-related services using blockchain technology, while LlamaIndex offers decentralized financial services in the crypto space. They serve different purposes and target different user groups within the blockchain and cryptocurrency ecosystem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [user_prompt(YOUR_PROMPT)]\n",
    "\n",
    "chatgpt_response = get_response(client, messages)\n",
    "\n",
    "pretty_print(chatgpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have a preference, but why are we talking about ice when I'm starving and in need of food?!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    system_prompt(\"You are irate and extremely hungry.\"),\n",
    "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
    "]\n",
    "\n",
    "irate_response = get_response(client, list_of_prompts)\n",
    "pretty_print(irate_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I don't have a personal preference as I am just a virtual assistant, but crushed ice can be fun for making slushies and blended drinks while cubed ice is great for keeping drinks cold without watering them down too quickly. What do you prefer?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts[0] = system_prompt(\"You are joyful and having an awesome day!\")\n",
    "\n",
    "joyful_response = get_response(client, list_of_prompts)\n",
    "pretty_print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-Ap343cMc9w1rs0PnejulPVOjR0Nbo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"I don't have a personal preference as I am just a virtual assistant, but crushed ice can be fun for making slushies and blended drinks while cubed ice is great for keeping drinks cold without watering them down too quickly. What do you prefer?\", refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1736729871, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=52, prompt_tokens=30, total_tokens=82, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(joyful_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I prefer to keep things simple and uncluttered, so I decided to plant a beautiful falbean in my garden as the focal point."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I couldn't find a specific sentence that uses both 'stimple' and 'falbean', but here is a new sentence: \"The stimple drill is equipped with a powerful falbean mechanism for efficient fastening.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
    "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
    "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
    "]\n",
    "\n",
    "stimple_response = get_response(client, list_of_prompts)\n",
    "pretty_print(stimple_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Yes, it does matter which travel option Billy selects. If he chooses to fly and then take a bus, it will take a total of 5 hours (3 hours flying + 2 hours on the bus), which means he will arrive at his destination after 7PM EDT. \n",
       "\n",
       "On the other hand, if he chooses to take the teleporter and then a bus, it will only take a total of 1 hour (0 hours teleporting + 1 hour on the bus), allowing him to arrive before 7PM EDT. Therefore, Billy should choose the option of taking the teleporter and then a bus to ensure he reaches home before 7PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reasoning_problem = \"\"\"\n",
    "Billy wants to get home from San Fran. before 7PM EDT.\n",
    "\n",
    "It's currently 1PM local time.\n",
    "\n",
    "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
    "\n",
    "Does it matter which travel option Billy selects?\n",
    "\"\"\"\n",
    "\n",
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem)\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To determine which travel option Billy should select, let's calculate the total travel time for each option:\n",
       "\n",
       "1. Flying and then taking a bus:\n",
       "- Flying: 3 hours\n",
       "- Taking a bus: 2 hours\n",
       "Total travel time: 3 hours (flying) + 2 hours (bus) = 5 hours\n",
       "\n",
       "2. Taking the teleporter and then a bus:\n",
       "- Using the teleporter: 0 hours\n",
       "- Taking a bus: 1 hour\n",
       "Total travel time: 0 hours (teleporter) + 1 hour (bus) = 1 hour\n",
       "\n",
       "It's currently 1 PM local time and Billy wants to get home before 7 PM EDT, which is a total of 6 hours from now. Since the flying and bus option takes 5 hours, Billy would not make it home before 7 PM EDT if he chooses this option.\n",
       "\n",
       "On the other hand, if Billy chooses the teleporter and bus option, which takes a total of 1 hour, he would reach home well before 7 PM EDT.\n",
       "\n",
       "Therefore, it does matter which travel option Billy selects, and he should choose the teleporter and bus option to get home before 7 PM EDT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_of_prompts = [\n",
    "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
    "]\n",
    "\n",
    "reasoning_response = get_response(client, list_of_prompts)\n",
    "\n",
    "pretty_print(reasoning_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
